{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "js5njRNyTOpr"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KravitzLab/KreedLabWiki/blob/main/Kravitz_PCA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PCA plotting for PsyGene Mice Behavior"
      ],
      "metadata": {
        "id": "dGdV92l6yTac"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install igraph\n",
        "#!pip install leidenalg\n",
        "#!pip install scanpy"
      ],
      "metadata": {
        "collapsed": true,
        "id": "3LAzYknE2CrX"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "id": "K6g2Cvhu6jAY",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "#@title # Import Libraries\n",
        "\n",
        "####### Import Needed Libraries #######\n",
        "# Libraries for Machine Learnings/Modeling\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.decomposition import PCA\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import adjusted_rand_score, normalized_mutual_info_score\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "# Libraries for importing\n",
        "import zipfile\n",
        "from google.colab import files # creates the ability for display uploads\n",
        "import tempfile\n",
        "import io\n",
        "import os\n",
        "# Data Frame libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for merging\n",
        "from functools import reduce\n",
        "# UMAP\n",
        "import umap\n",
        "# Plotting\n",
        "import seaborn as sns\n",
        "import matplotlib.patches as mpatches\n",
        "import matplotlib.pyplot as plt\n",
        "# Clustering\n",
        "from sklearn.neighbors import kneighbors_graph\n",
        "import igraph as ig\n",
        "import leidenalg\n",
        "import scanpy as sc\n",
        "\n",
        "\n",
        "from decorator import DEF\n",
        "\n",
        "import ipywidgets as widgets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Define functions used through out the script\n",
        "\n",
        "####### DEFINE ALL FUNCTIONS #######\n",
        "\n",
        "### Session type Extraction ###\n",
        "# This code will extract the \"session_type\" column from each file\n",
        "def extract_session_type(df, file_name, fallback=\"Unknown\"):\n",
        "    # Follow Naming conventions so far\n",
        "    session_types = [\"weight\", \"bandit100\", \"fr1\", \"beam\", \"bandit80\",\n",
        "                     \"_pr_\", # we have to define pr surrounded by underscores to prevent random pr's\n",
        "                     \"individual_behavoir\", \"social_interaction\", \"nesting\"]\n",
        "    \"\"\"Read 'Session_Type ' or variants; return first non-empty value.\"\"\"\n",
        "    file_name = file_name.lower()\n",
        "    print(f\"Extracting session type for file: {file_name}\")\n",
        "    try:\n",
        "        #df = pd.read_csv(csv_path, sep=None, engine='python', dtype=str)\n",
        "        df = df\n",
        "        df.columns = [c.strip() for c in df.columns]\n",
        "        lower = {c.casefold(): c for c in df.columns}\n",
        "        # Loop through common session like column names\n",
        "        for cand in [\"session_type\", \"session type\", \"sessiontype\", \"session\"]:\n",
        "            if cand in lower:\n",
        "                col = lower[cand]\n",
        "                vals = df[col].dropna().astype(str).str.strip()\n",
        "                vals = vals[vals.ne(\"\")]\n",
        "                if not vals.empty:\n",
        "                    return vals.iloc[0].lower()\n",
        "            else:\n",
        "              # use the file name to determine what session type it is\n",
        "              matches = [c for c in session_types if c.lower() in file_name.lower()]\n",
        "              return(matches[0])\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading {csv_path}: {e}\")\n",
        "    return fallback\n",
        "\n",
        "### Retrieve dataframe profiles ###\n",
        "# expects a dataframe as an argument\n",
        "def df_prof(df):\n",
        "  # Get Dimensions\n",
        "  print(f\"Number of rows: {df.shape[0]}\")\n",
        "  print(f\"Number of columns: {df.shape[1]}\")\n",
        "\n",
        "  # Data types\n",
        "  df_prof = df.dtypes.rename(\"DataType\")\n",
        "\n",
        "  # Null percentages\n",
        "  df_null = (df.isna().sum() / len(df) * 100).rename(\"Percent_Null\")\n",
        "\n",
        "  # unique counts\n",
        "  df_uniq = df.nunique().rename(\"Number_Unique\")\n",
        "\n",
        "  # Merge into one DataFrame\n",
        "  merged_df = pd.concat([df_prof, df_null, df_uniq], axis=1).reset_index()\n",
        "  merged_df.columns = [\"Column\", \"DataType\", \"Percent_Null\", \"Number_Unique\"]\n",
        "  print(merged_df)\n",
        "\n",
        "\n",
        "\n",
        "### Define easy function for finding mode ###\n",
        "def mode_or_nan(x):\n",
        "    # handle ties or empty groups safely\n",
        "    return x.mode().iloc[0] if not x.mode().empty else None\n",
        "\n",
        "\n",
        "### Aggregate Session Dataframes ###\n",
        "# expects dictionary containing dataframes\n",
        "# Requires \"Mouse_ID\" Column\n",
        "# Will iterate through dataframes within the session dictionary passed\n",
        "#     and aggregate by Mouse_ID and ensure only 1 mouse is represented.\n",
        "# define this as a function for better readbility and to easily adjust\n",
        "#   drop lists\n",
        "def aggregate_session_types(session_dfs):\n",
        "    # Iterate across the session dictionary\n",
        "    for s, d in session_dfs.items():\n",
        "        print(f\"Aggregating Session Type: {s}\")\n",
        "\n",
        "        # Check to see if primary key is same length as number of rows in dataframe\n",
        "        if len(d) == d[\"Mouse_ID\"].nunique():\n",
        "            print(\"Number of Mice equal to Length of dataframe: No Aggregation Needed\")\n",
        "            continue\n",
        "\n",
        "        # Define numeric and categorical columns\n",
        "        numeric_cols = d.select_dtypes(include=['int64', 'float64']).columns\n",
        "        object_cols = d.select_dtypes(include=['object', 'category', 'datetime64']).columns\n",
        "\n",
        "        # perform aggregation\n",
        "        agg_dict = {\n",
        "            **{col: 'mean' for col in numeric_cols},\n",
        "            **{col: mode_or_nan for col in object_cols}\n",
        "        }\n",
        "\n",
        "        session_dfs[s] = (d.groupby('Mouse_ID', dropna=False, as_index=False)\n",
        "                          .agg(agg_dict))\n",
        "    return session_dfs\n",
        "\n",
        "\n",
        "\n",
        "### Clean Session Dataframes ###\n",
        "# Very fragile and expects the user to do manual investigation\n",
        "# could add detection for columns with null values\n",
        "# define this as a function for better readbility and to easily adjust\n",
        "#   drop lists\n",
        "def clean_session_types(session_dfs, all_drops, percent_drop = 5):\n",
        "  # Interate through large table and remove\n",
        "  for s, d in session_dfs.items():\n",
        "    print(f\"Cleaning Session Type: {s}\")\n",
        "\n",
        "    # remove \"$\" from column names\n",
        "    session_dfs[s].columns = session_dfs[s].columns.str.replace('$', '')\n",
        "\n",
        "    ### Conduct general data cleaning ###\n",
        "    session_dfs[s] = session_dfs[s].drop(columns = all_drops, errors='ignore')\n",
        "\n",
        "\n",
        "    ### set to lowercase ###\n",
        "    session_dfs[s].columns = session_dfs[s].columns.str.lower()\n",
        "\n",
        "    ### Drop based on nullness ###\n",
        "    # Calculate the percentage of null values for each column\n",
        "    null_percentages = session_dfs[s].isnull().sum() * 100 / len(session_dfs[s])\n",
        "\n",
        "    # ensure that key columns are not dropped\n",
        "    key_cols = [\"mouse_id\", \"sex\", \"genotype\", \"gene\"]\n",
        "\n",
        "    # Identify columns where the null percentage is greater than 5%\n",
        "    columns_to_drop = null_percentages[null_percentages > percent_drop].index\n",
        "\n",
        "    # Remove key columns from the drop list\n",
        "    columns_to_drop = [col for col in columns_to_drop if col not in key_cols]\n",
        "\n",
        "    # Drop the identified columns from the DataFrame\n",
        "    session_dfs[s] = session_dfs[s].drop(columns=columns_to_drop)\n",
        "\n",
        "    ### Add suffix if doesnt exist ###\n",
        "    # Columns to not add suffix\n",
        "    exceptions = [\"mouse_id\", \"sex\", \"genotype\", \"gene\", \"session_type\"]\n",
        "\n",
        "    session = session_dfs[s][\"session_type\"].iloc[0]\n",
        "    session_dfs[s].rename(\n",
        "    columns=lambda c: f\"{c}_{session}\"\n",
        "                      if (c not in exceptions and c not in exceptions and not c.endswith(f\"_{session}\"))\n",
        "                      else c,\n",
        "    inplace=True\n",
        "    )\n",
        "    \"\"\"\n",
        "    ### Clean bandit80 specific data ###\n",
        "    if s.lower() in [\"bandit80\", \"bandit100\"]:\n",
        "      session_dfs[s] = session_dfs[s].drop(columns = bandit_drops, errors='ignore')\n",
        "      # Ensure date times are correct data type\n",
        "      session_dfs[s]['FED_StartDate'] = pd.to_datetime(session_dfs[s]['FED_StartDate'])\n",
        "\n",
        "    ### Clean weight specific data ###\n",
        "    if s.lower() in [\"weight\"]:\n",
        "\n",
        "      session_dfs[s] = session_dfs[s].drop(columns = weight_drops, errors='ignore')\n",
        "      # Ensure date times are correct data type\n",
        "      session_dfs[s]['DOB'] = pd.to_datetime(session_dfs[s]['DOB'])\n",
        "\n",
        "    # remove session type after all the cleaning\n",
        "    session_dfs[s] = session_dfs[s].drop(columns = \"session_type\", errors='ignore')\n",
        "  \"\"\"\n",
        "  ### Return the session_dfs ###\n",
        "  return(session_dfs)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "qyWLkGb6tRNG",
        "cellView": "form"
      },
      "execution_count": 145,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Import Wanted Data Files\n",
        "#@markdown Code based on Chantelle Murelle and Sebastian Alves\\\n",
        "#@markdown The following code will iteritvly read all csv's within the\n",
        "#@markdown zipped file and seperate them into a dictionary based on the various\n",
        "#@markdown recordings. this will be used for processing the data in a session type manner.\\\n",
        "#@markdown This could include L3 for all various recordings:\n",
        "#@markdown 1.   Weight\n",
        "#@markdown 2.   Bandit100_0\n",
        "#@markdown 3.   FR1\n",
        "#@markdown 4.   BEAM\n",
        "#@markdown 5.   Bandit80_20\n",
        "#@markdown 6.   PR\n",
        "#@markdown 7.   Individual Behavoir\n",
        "#@markdown 8.   Social Interaction\n",
        "#@markdown 9.   Nesting\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####### Import Wanted Data Files ########\n",
        "# Code based on Chantelle Murelle and Sebastian Alves\n",
        "\n",
        "### reset cache ###\n",
        "# Reset caches to avoid duplicates if you re-run this cell\n",
        "session_dict, loaded_files, session_types = [], [], []\n",
        "\n",
        "### Define the upload UI element ###\n",
        "uploaded = files.upload()\n",
        "\n",
        "### Create Data dictonary and containers ###\n",
        "session_dict = {} # for storing by session type\n",
        "loaded_files = [] # for files\n",
        "\n",
        "### Loop through the uploaded files ###\n",
        "# This expects a zippped file exclusivly\n",
        "for name, data in uploaded.items():\n",
        "  with zipfile.ZipFile(io.BytesIO(data)) as zf: # use zipfile to \"unzip\"\n",
        "    for zi in zf.infolist(): # iterate through the zipped files\n",
        "\n",
        "\n",
        "      # skip non handled file types\n",
        "      # Skip non-csv/xlsx files\n",
        "      if not (zi.filename.endswith(\".csv\") or zi.filename.endswith(\".xlsx\")):\n",
        "        print(f\"Skipping: {zi.filename}\")\n",
        "        continue\n",
        "      print(f\"\\nProcessing: {zi.filename}\")\n",
        "\n",
        "      # define filetype\n",
        "      file_type = \"csv\" if zi.filename.endswith(\".csv\") else \"xlsx\"\n",
        "\n",
        "      # read the file in\n",
        "      file_data = zf.read(zi)\n",
        "\n",
        "      # write temporary file into memory\n",
        "      suffix = \".csv\" if file_type == \"csv\" else \".xlsx\"\n",
        "      with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=suffix, delete=False) as tmp:\n",
        "        tmp.write(file_data)\n",
        "        tmp_path = tmp.name\n",
        "\n",
        "      # Define try catch statment and read into memory\n",
        "      try:\n",
        "        if file_type == \"csv\":\n",
        "          df = pd.read_csv(tmp_path)\n",
        "        else:\n",
        "          df = pd.read_excel(tmp_path)\n",
        "        #df[\"source_file\"] = zi.filename\n",
        "        session_type = extract_session_type(df, zi.filename)\n",
        "        # rewrite session\n",
        "        df[\"session_type\"] = session_type\n",
        "        print(session_type)\n",
        "\n",
        "        # Add to the data dictionary based on session type\n",
        "        if session_type not in session_dict:\n",
        "          session_dict[session_type] = []\n",
        "        session_dict[session_type].append(df)\n",
        "      except Exception as e: # throw error on failure to load\n",
        "        print(f\"Error loading {zi.filename}: {e}\")\n",
        "      finally: # remove the temporary path\n",
        "        os.remove(tmp_path)\n",
        "\n",
        "\n",
        "\n",
        "### Combine dataframes for each session ###\n",
        "# Convert each list of dfs into one merged dataframe per session type\n",
        "session_dfs = {k: pd.concat(v, ignore_index=True) for k, v in session_dict.items()}\n",
        "\n",
        "session_dfs.pop('weight', None)\n",
        "#session_dfs.pop('bandit100', None)\n",
        "session_dfs.pop('beam', None)\n",
        "session_dfs.pop('si', None)\n",
        "\n",
        "#print(session_dfs['bandit100'])\n",
        "\n",
        "### Return overarcing view of all sessiont types ###\n",
        "for s, d in session_dfs.items():\n",
        "    print(f\"\\nSession '{s}' → {d.shape[0]} rows, {d.shape[1]} cols\")\n",
        "    df_prof(session_dfs[s])"
      ],
      "metadata": {
        "id": "8pJCTyGLzH2S",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Begin to clean data.\n",
        "# @markdown Define what columns for what session types will be dropped.\n",
        "# @markdown This should be based on nullness and should seek to keep as\n",
        "# @markdown large an N as possible. Additionally, columns that are believe\n",
        "# @markdown to add little to no context could be dropped.\n",
        "\n",
        "####### BEGIN DATA CLEANING AND SCRUBBING #######\n",
        "\n",
        "\n",
        "# drop known problem columns\n",
        "all_drops = [\"filename\", \"filename_md\", \"Session_type\", \"match_status\",\n",
        "               \"Primary\", \"Autoexcluder\", \"Gene_ID\"]\n",
        "\n",
        "\n",
        "\n",
        "dfs_agg = aggregate_session_types(session_dfs)\n",
        "\n",
        "\n",
        "dfs_clean = clean_session_types(session_dfs=dfs_agg,\n",
        "                                all_drops=all_drops)\n",
        "\n",
        "\n",
        "### Return overarcing view of all sessiont types ###\n",
        "for s, d in dfs_clean.items():\n",
        "    print(f\"\\nSession '{s}' → {d.shape[0]} rows, {d.shape[1]} cols\")\n",
        "    df_prof(dfs_clean[s])\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "WZ6yTNtP7X-D",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Merge and Prepare Columns\n",
        "# @markdown Continues cleaning and data formatting including:\n",
        "# @markdown 1. Remove any rows that have any null values present.\n",
        "# @markdown 2. Number of Days columns (if date times are present).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "####### Merge the list of dataframes #######\n",
        "\n",
        "# Define the features that will be merged on\n",
        "merge_keys = [\"mouse_id\", \"gene\", \"sex\", \"genotype\"]\n",
        "\n",
        "# Define columns for metadata\n",
        "meta_rows = []\n",
        "\n",
        "# ensure all key columns are normalized\n",
        "for s, d in dfs_clean.items():\n",
        "    dfs_clean[s][merge_keys] = dfs_clean[s][merge_keys].astype(str).apply(lambda col: col.str.upper())\n",
        "\n",
        "    # Build meta data file\n",
        "    # extract the session type (assume same for all rows)\n",
        "    session = dfs_clean[s][\"session_type\"].iloc[0]\n",
        "    # take all columns *except* session_type\n",
        "    cols = dfs_clean[s].columns.drop(\"session_type\")\n",
        "    # add metadata rows\n",
        "    for c in cols:\n",
        "        meta_rows.append({\"column_name\": c, \"session_type\": session})\n",
        "\n",
        "# Create the meta datafrane linking all session types to features\n",
        "df_meta = pd.DataFrame(meta_rows)\n",
        "df_meta = df_meta[~df_meta[\"column_name\"].isin(merge_keys)]\n",
        "\n",
        "df_meta = df_meta[['session_type', 'column_name']]\n",
        "print(f\"Meta data frame: \\n{df_meta}\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Merge files\n",
        "merged = reduce(\n",
        "    lambda left, right: pd.merge(\n",
        "        left, right,\n",
        "        on=merge_keys,\n",
        "        how=\"outer\",\n",
        "        suffixes=(\"\", \"_dup\")\n",
        "    ),\n",
        "    dfs_clean.values()\n",
        ")\n",
        "\n",
        "\n",
        "### Remove any rows that contain null values ###\n",
        "# Replace textual 'nan' with real NaN and assign\n",
        "replaced = merged.replace('nan', np.nan)\n",
        "# Keep only rows that have NO nulls in any column\n",
        "df_all = replaced[~replaced.isnull().any(axis=1)].reset_index(drop=True)\n",
        "\n",
        "\n",
        "### Deal with the dates ###\n",
        "# by representing how old each mouse is (days) for various tasks\n",
        "dob_col = df_all.columns[df_all.columns.str.contains(\"dob\")]\n",
        "if not dob_col.empty:\n",
        "  # Extract column\n",
        "  dob_col = dob_col[0]\n",
        "  print(f\"Found DOB Column: {dob_col} \\nCalculating days old for tasks\")\n",
        "  fed_starts = df_all.columns[df_all.columns.str.contains(\"FED_StartDate\")]\n",
        "\n",
        "  # Convert DOB column once to datetime\n",
        "  dob_dt = pd.to_datetime(df_all[dob_col], errors='coerce')\n",
        "\n",
        "  # Loop through each FED_StartDate column and calculate days difference\n",
        "  for fed_col in fed_starts:\n",
        "    new_col_name = f\"days_old_for_{fed_col}\"\n",
        "    df_all[new_col_name] = (\n",
        "        pd.to_datetime(df_all[fed_col], errors='coerce') - dob_dt\n",
        "        ).dt.days\n",
        "    print(f\"→ Created column: {new_col_name}\")\n",
        "\n",
        "\n",
        "### Remove any datetime columns ###\n",
        "datetime_cols = df_all.select_dtypes(include=['datetime64']).columns\n",
        "df_all.drop(columns = datetime_cols, inplace=True)\n",
        "\n",
        "\n",
        "### Check the data ###\n",
        "for col in df_all.columns:\n",
        "  # Data types\n",
        "  col_type = df_all[col].dtypes\n",
        "  # Null percentages\n",
        "  col_null = (df_all[col].isna().sum() / (len(df_all[col]) * 100))\n",
        "  print(f\"Name: {col} | Datatype: {col_type} | Percent null: {col_null} \")\n",
        "#print(df_all.columns.values)\n",
        "#df_prof(df_all)\n",
        "#df_all.head()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# ---------- Save CSV ----------\n",
        "#csv_name = f\"PRmetrics_{datetime.now():%Y%m%d_%H%M%S}.csv\"\n",
        "csv_name = f\"all_merged_dataframes.csv\"\n",
        "\n",
        "down_df = df_all.to_csv(csv_name, index=False)\n",
        "#display(HTML(f\"<b>✓ Saved PR metrics CSV to:</b> <code>{csv_name}</code>\"))\n",
        "\n",
        "try:\n",
        "    from google.colab import files as gfiles\n",
        "except Exception:\n",
        "    gfiles = None\n",
        "\n",
        "# Optional download button (works in Colab)\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(csv_name)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(csv_name)}</code>…\"\n",
        "        gfiles.download(csv_name)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{csv_name}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JQrmiaETytxY",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Prepare data for multiple ML approaches\n",
        "# @markdown Should prepare data in theory for a RandomForestModel\\\n",
        "# @markdown This will include:\n",
        "# @markdown 1. One-hot Encoding\n",
        "# @markdown 2. Split into Training and Testing datasets\n",
        "# @markdown 3. Normalization of data\n",
        "\n",
        "\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "### Define the classifier for prediction ###\n",
        "pred_col = [\"gene\"]\n",
        "\n",
        "### Define known categorical columns ###\n",
        "known_objects = [\"sex\", \"genotype\"]\n",
        "\n",
        "### Remove date-time columns ###\n",
        "datetime_cols = df.select_dtypes(include=['datetime64[ns]']).columns\n",
        "# Drop the datetime columns\n",
        "df_no_datetime = df.drop(columns=datetime_cols)\n",
        "\n",
        "# Drop problem columns\n",
        "columns_to_drop = [col for col in df_all.columns if\n",
        "                   not (col.startswith('stim_partner')\n",
        "                   or col.startswith('stim_weight')\n",
        "                   or col.startswith('fed3_')\n",
        "                   or col.startswith('beam_')\n",
        "                   or col.startswith('fed_')\n",
        "                   or col.startswith('dob_')\n",
        "                   or col.startswith('session_type')\n",
        "                   or col.startswith('time_of_min_')\n",
        "                   or col.startswith('time_of_max_peak_')\n",
        "                   or col.startswith('beam_time_of')\n",
        "                   or col.startswith('unnamed')\n",
        "                   or col.startswith('weight')\n",
        "                   )]\n",
        "df_all = df_all[columns_to_drop]\n",
        "\n",
        "\n",
        "\n",
        "### Edit the df Meta table\n",
        "# Define patterns to drop\n",
        "patterns_to_drop = [\n",
        "    'stim_partner',\n",
        "    'stim_weight',\n",
        "    'fed3_',\n",
        "    'beam_',\n",
        "    'fed_',\n",
        "    'dob_',\n",
        "    'session_type',\n",
        "    'time_of_min_',\n",
        "    'time_of_max_peak_',\n",
        "    'beam_time_of',\n",
        "    'unnamed',\n",
        "    'weight'\n",
        "]\n",
        "\n",
        "# Keep rows where the column **does not start with any of these patterns**\n",
        "df_meta = df_meta[\n",
        "    ~df_meta['column_name'].astype(str).str.startswith(tuple(patterns_to_drop))\n",
        "]\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Cast categorical columns ###\n",
        "df_all[known_objects] = df_all[known_objects].astype('object')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "### Begin encoding ###\n",
        "\n",
        "# encode multiclass prediciton\n",
        "le = LabelEncoder()\n",
        "df_all[\"gene\"] = le.fit_transform(df_all[\"gene\"])\n",
        "\n",
        "\n",
        "# Define ID columns (Columns to Drop)\n",
        "id_cols = [\"mouse_id\"]\n",
        "id_cols.extend(pred_col)\n",
        "\n",
        "\n",
        "# Define X and y\n",
        "y = df_all[pred_col]\n",
        "print(y)\n",
        "\n",
        "X = df_all.drop(columns=id_cols)\n",
        "\n",
        "# Train/test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
        "\n",
        "# Identify categorical columns\n",
        "cat_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
        "\n",
        "# One-hot encode categorical features\n",
        "print(f\"One-hot encoding the following columns: {list(cat_cols)}\")\n",
        "X_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True, dtype=bool)\n",
        "X_test = pd.get_dummies(X_test, columns=cat_cols, drop_first=True, dtype=bool)\n",
        "\n",
        "# Align columns between train and test\n",
        "X_train, X_test = X_train.align(X_test, join='left', axis=1, fill_value=0)\n",
        "\n",
        "# Scale data\n",
        "# find numeric\n",
        "num_cols = X_train.select_dtypes(include=['float64', 'int64']).columns\n",
        "# find boolean\n",
        "bool_cols = X_train.select_dtypes(include=['bool']).columns\n",
        "\n",
        "\n",
        "# fit to only training\n",
        "scaler.fit(X_train[num_cols])\n",
        "\n",
        "X_train[num_cols] = scaler.transform(X_train[num_cols])\n",
        "X_test[num_cols] = scaler.transform(X_test[num_cols])\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Run lasso to determine linked features\n",
        "lasso_model = Lasso(alpha=0.1)\n",
        "lasso_model.fit(X_train, y_train)\n",
        "\n",
        "coefficients = pd.Series(lasso_model.coef_, index=X_train.columns)\n",
        "selected_features = coefficients[coefficients != 0].index.tolist()\n",
        "print(selected_features)\n",
        "\n",
        "# Return only selected features\n",
        "X_train = X_train[selected_features]\n",
        "X_test = X_test[selected_features]\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Glimpse training data\n",
        "X_train.head()\n",
        "#print(df_prof(X_train))\n",
        "\n",
        "\n",
        "### create the download for a metafile ###\n",
        "csv_name2 = f\"meta_dataframe.csv\"\n",
        "down_df2 = df_meta.to_csv(csv_name2, index=False)\n",
        "\n",
        "btn = widgets.Button(description=f\"Download {os.path.basename(csv_name2)}\", icon=\"download\")\n",
        "status = widgets.HTML()\n",
        "def _dl(_):\n",
        "    if gfiles is not None:\n",
        "        status.value = f\"Starting download: <code>{os.path.basename(csv_name2)}</code>…\"\n",
        "        gfiles.download(csv_name2)\n",
        "    else:\n",
        "        status.value = f\"Saved locally at <code>{csv_name2}</code>.\"\n",
        "display(btn, status)\n",
        "btn.on_click(_dl)"
      ],
      "metadata": {
        "id": "sdZFggMYA_mG",
        "cellView": "form",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Tune hyperparameters for Random Forest Modeling\n",
        "\n",
        "# tune hyperparameters\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "import sklearn\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "\n",
        "# Define RF\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200],\n",
        "    'max_depth': [None, 10, 20],\n",
        "    'min_samples_split': [2, 5],\n",
        "    'min_samples_leaf': [1, 2]\n",
        "}\n",
        "\n",
        "# 1) Your Code Below\n",
        "# initialize the stratified fold\n",
        "cv = StratifiedKFold(n_splits = 5, shuffle = True, random_state = 42)\n",
        "\n",
        "\n",
        "grid_search = GridSearchCV(\n",
        "    estimator = rf,\n",
        "    param_grid = param_grid_rf,\n",
        "    scoring = \"roc_auc_ovr\",\n",
        "    cv = cv,\n",
        "    n_jobs = 1\n",
        ")\n",
        "\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "#grid_search\n",
        "print(grid_search.best_params_)\n",
        "print(grid_search.best_score_)\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Ca8kp0odEzvD",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Fit RF model onto data using best parameters\n",
        "\n",
        "# refit using best parameters\n",
        "# Get the best parameters\n",
        "best_params = grid_search.best_params_\n",
        "\n",
        "# Re-initialize model with best parameters\n",
        "final_rf = RandomForestClassifier(**best_params, random_state=42)\n",
        "\n",
        "# Fit the model on the entire training data\n",
        "final_rf.fit(X_train, y_train)"
      ],
      "metadata": {
        "id": "yktMvy8ZLc4Q",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title # Perform predictions and check prediction scores\n",
        "\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Predictions\n",
        "y_pred = final_rf.predict(X_test)\n",
        "y_prob = final_rf.predict_proba(X_test)\n",
        "\n",
        "# Accuracy\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "\n",
        "# ROC-AUC\n",
        "print(\"ROC-AUC (OvR):\", roc_auc_score(y_test, y_prob, multi_class='ovr'))\n",
        "\n",
        "# Classification report\n",
        "print(\"\\nClassification Report:\")\n",
        "print(classification_report(y_test, y_pred))\n",
        "\n",
        "\n",
        "# Confusion matrix heatmap\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "plt.figure(figsize=(8, 6))\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.xlabel(\"Predicted\")\n",
        "plt.ylabel(\"True\")\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "-hs8bxUGMZUX",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Find and view most important features\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "feat_imp = pd.DataFrame({\n",
        "    \"feature\": X_train.columns,\n",
        "    \"importance\": final_rf.feature_importances_\n",
        "})\n",
        "\n",
        "feat_imp = feat_imp.sort_values(\"importance\", ascending=False)\n",
        "\n",
        "print(\"Most Important features for prediction\")\n",
        "print(feat_imp.head(15))\n",
        "\n",
        "print(f\"\\nLeast important features for prediction\")\n",
        "print(feat_imp.tail(15))\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "S8MytPF0NQxT",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Correlation Matrix\n",
        "\n",
        "\n",
        "\n",
        "# Build groups for features linked to\n",
        "\n",
        "\n",
        "### Drop Keys and object columns ###\n",
        "pca_df = df_all.drop(columns = merge_keys)\n",
        "#foo = pd.concat([X_train, X_test])\n",
        "drops = pca_df.select_dtypes(include=['object', 'bool']).columns\n",
        "pca_df.drop(columns = drops, inplace=True)\n",
        "\n",
        "\n",
        "# Plot correlation matrix to understand highly linked features\n",
        "# Correlation matrix\n",
        "corr = pca_df.corr()\n",
        "\n",
        "groups = df_meta[\"session_type\"].unique()\n",
        "palette = sns.color_palette(\"tab10\", n_colors=len(groups))\n",
        "\n",
        "# Build a mapping: group -> color\n",
        "group_colors = {}\n",
        "for g, color in zip(groups, palette):\n",
        "    group_colors[g] = color\n",
        "\n",
        "\n",
        "\n",
        "col_colors = [group_colors[g] for g in df_meta[\"session_type\"]]\n",
        "\n",
        "\n",
        "\n",
        "sns.clustermap(corr, row_colors=col_colors, col_colors=col_colors,\n",
        "               row_cluster=False, col_cluster=False, cmap=\"coolwarm\", annot=False)\n",
        "\n",
        "\n",
        "\n",
        "handles = [mpatches.Patch(color=color, label=grp) for grp, color in group_colors.items()]\n",
        "plt.legend(handles=handles, bbox_to_anchor=(1,1))\n",
        "\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\"\"\" This kinda works\n",
        "# Create a feature -> group mapping\n",
        "feature_to_group = dict(zip(df_meta[\"column_name\"], df_meta[\"session_type\"]))\n",
        "\n",
        "# Replace tick labels with group names\n",
        "xlabels = [feature_to_group[col] for col in corr.columns]\n",
        "ylabels = [feature_to_group[row] for row in corr.index]\n",
        "\n",
        "plt.figure(figsize=(6,6))\n",
        "sns.heatmap(corr, xticklabels=xlabels, yticklabels=ylabels,\n",
        "            cmap=\"coolwarm\", annot=False, fmt=\".2f\")\n",
        "plt.xticks(rotation=45, ha='right')\n",
        "plt.yticks(rotation=0)\n",
        "plt.title(\"Feature-level correlation with group x-axis labels\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "#Drops high correlation data\n",
        "# Find upper correlatives\n",
        "threshold = 0.95\n",
        "corr_matrix_abs = corr.corr().abs()\n",
        "upper_tri = corr_matrix_abs.where(pd.DataFrame(np.triu(np.ones(corr_matrix_abs.shape), k=1),\n",
        "                                                columns=corr_matrix_abs.columns,\n",
        "                                           index=corr_matrix_abs.index).astype(bool))\n",
        "to_drop = [column for column in upper_tri.columns if any(upper_tri[column] > threshold)]\n",
        "print(f\"Highly correlated features: {to_drop}\")\n",
        "#pca_df = pca_df.drop(columns=to_drop)\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "HAL0bio8yLBq",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Attempt UMAP\n",
        "\n",
        "### Drop Keys and object columns ###\n",
        "umap_df = df_all.copy()\n",
        "\n",
        "# drop rows with wt\n",
        "geno_type = \"HET\"\n",
        "umap_df = umap_df[umap_df['genotype'] == geno_type]\n",
        "\n",
        "# Find sex for labeling later\n",
        "gender_labels = umap_df['sex']\n",
        "\n",
        "# get gene labels to plot by gene\n",
        "labels_encode = umap_df[\"gene\"]\n",
        "labels = le.inverse_transform(umap_df[\"gene\"])\n",
        "\n",
        "# Drop the columns that are identifiers\n",
        "umap_df = umap_df.drop(columns = merge_keys)\n",
        "print(merge_keys)\n",
        "\n",
        "# Ensure columns have been dropped\n",
        "drops = umap_df.select_dtypes(include=['object', 'bool']).columns\n",
        "umap_df.drop(columns = drops, inplace=True)\n",
        "\n",
        "# features to scale\n",
        "features = umap_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "### Standardize the scale ###\n",
        "x = umap_df.loc[:, features].values\n",
        "x = StandardScaler().fit_transform(x)\n",
        "\n",
        "feat_cols = ['feature'+str(i) for i in range(x.shape[1])]\n",
        "umap_df = pd.DataFrame(x, columns=feat_cols)\n",
        "\n",
        "### Create the UMAP embeddings ###\n",
        "reducer = umap.UMAP(\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    metric=\"euclidean\",\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "embedding = reducer.fit_transform(umap_df)\n",
        "\n",
        "\n",
        "# Convert gene labels to colors\n",
        "unique_genes = np.unique(labels)\n",
        "palette = sns.color_palette(\"Pastel2\", len(unique_genes))\n",
        "gene_color_map = dict(zip(unique_genes, palette))\n",
        "\n",
        "# Male / Female masks\n",
        "male_mask = (gender_labels == 'M')\n",
        "female_mask = (gender_labels == 'F')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "### Male UMAP = filled points ###\n",
        "plt.scatter(\n",
        "    embedding[male_mask, 0],\n",
        "    embedding[male_mask, 1],\n",
        "    s=60,\n",
        "    c=[gene_color_map[g] for g in labels[male_mask]],\n",
        "    #edgecolors='black',\n",
        "    #linewidths=0.5,\n",
        "    label=\"Male (filled)\"\n",
        ")\n",
        "\n",
        "### female UMAP = hollow points ###\n",
        "plt.scatter(\n",
        "    embedding[female_mask, 0],\n",
        "    embedding[female_mask, 1],\n",
        "    s=60,\n",
        "    facecolors='none',\n",
        "    edgecolors=[gene_color_map[g] for g in labels[female_mask]],\n",
        "    linewidths=3.0,\n",
        "    label=\"Female (hollow)\"\n",
        ")\n",
        "\n",
        "# Gene color legend\n",
        "for g in unique_genes:\n",
        "    plt.scatter([], [], color=gene_color_map[g], label=g)\n",
        "plt.legend(title=\"Gene\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "### Plot the UMAP combied ###\n",
        "plt.title(f\"UMAP: Colored by Gene for {geno_type}, Filled by Sex\")\n",
        "plt.xlabel(\"UMAP-1\")\n",
        "plt.ylabel(\"UMAP-2\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "zG85itq-KCSZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title K-means & lidean clustering comparison for UMAP embeddings\n",
        "\n",
        "# Perform k-means clustering and save\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "kmean_labels = kmeans.fit_predict(umap_df)\n",
        "\n",
        "# PCA for visualization\n",
        "reducer = umap.UMAP(\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    metric=\"euclidean\",\n",
        "    random_state=42\n",
        ")\n",
        "clustering_kmeans_umap = reducer.fit_transform(umap_df)\n",
        "\n",
        "# Plot our pca data colored by clusters\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(clustering_kmeans_umap[:, 0], clustering_kmeans_umap[:, 1], c=kmean_labels,\n",
        "            cmap='Pastel2', s=30)\n",
        "plt.title('K-Means Clusters UMAP')\n",
        "plt.xlabel('UMAP 1')\n",
        "plt.ylabel('UMAP 2')\n",
        "\n",
        "# leiden - Build kNN graph\n",
        "k = 5\n",
        "knn_graph = kneighbors_graph(umap_df, k, mode='connectivity', include_self=False)\n",
        "sources, targets = knn_graph.nonzero()\n",
        "\n",
        "# Create igraph object\n",
        "g = ig.Graph(edges=list(zip(sources, targets)), directed=False)\n",
        "g.simplify()\n",
        "\n",
        "# Iterate thteough resplutions to find clustering for 5 groups\n",
        "for res in [0.2, 0.4, 0.6, 0.8, 1.0, 1.2]:\n",
        "    part = leidenalg.find_partition(\n",
        "        g,\n",
        "        leidenalg.RBConfigurationVertexPartition,\n",
        "        resolution_parameter=res\n",
        "    )\n",
        "    print(res, len(set(part.membership)))\n",
        "\n",
        "res = 1.2\n",
        "# run Leiden partitioning\n",
        "partition = leidenalg.find_partition(\n",
        "    g,\n",
        "    leidenalg.RBConfigurationVertexPartition,\n",
        "    resolution_parameter = res\n",
        ")\n",
        "leiden_labels = np.array(partition.membership)\n",
        "\n",
        "# Partition\n",
        "partition = leidenalg.find_partition(\n",
        "    g,\n",
        "    leidenalg.RBConfigurationVertexPartition,\n",
        "    resolution_parameter=1  # fewer clusters\n",
        ")\n",
        "\n",
        "# Plot the Leiden Clustering\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(\n",
        "    clustering_kmeans_umap[:,0],\n",
        "    clustering_kmeans_umap[:,1],\n",
        "    c=leiden_labels,\n",
        "    s=8,\n",
        "    alpha=0.8\n",
        ")\n",
        "plt.title(f\"Leiden Clustering (k = {k}, resolution = {res})\")\n",
        "plt.xlabel(\"UMAP 1\")\n",
        "plt.ylabel(\"UMAP 2\")\n",
        "plt.colorbar(label=\"Cluster\")\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "UbtuBN7J0TQN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title compare raw embeddings of labels to actual labels\n",
        "\n",
        "\n",
        "# Replace with your real label column\n",
        "true_labels = labels\n",
        "#print(f\"umap: {true_labels}\")\n",
        "kmean_labels_comp = le.inverse_transform(kmean_labels)\n",
        "#print(f\"kmeans: {le.inverse_transform(kmean_labels)}\")\n",
        "leiden_labels_comp = le.inverse_transform(leiden_labels)\n",
        "#print(f\"lendien: {le.inverse_transform(leiden_labels)}\")\n",
        "\n",
        "\n",
        "\n",
        "print(\"=== Adjusted Rand Index (ARI) ===\")\n",
        "print(\"KMeans vs Actual:\", adjusted_rand_score(true_labels, kmean_labels_comp))\n",
        "print(\"Leiden vs Actual:\", adjusted_rand_score(true_labels, leiden_labels_comp))\n",
        "\n",
        "print(\"\\n=== Normalized Mutual Information (NMI) ===\")\n",
        "print(\"KMeans vs Actual:\", normalized_mutual_info_score(true_labels, kmean_labels_comp))\n",
        "print(\"Leiden vs Actual:\", normalized_mutual_info_score(true_labels, leiden_labels_comp))\n",
        "\n",
        "\n",
        "cm_kmeans = pd.DataFrame(\n",
        "    confusion_matrix(true_labels, kmean_labels_comp),\n",
        "    index=[f\"True_{x}\" for x in sorted(set(true_labels))],\n",
        "    columns=[f\"KMeans_{x}\" for x in sorted(set(kmean_labels_comp))]\n",
        ")\n",
        "\n",
        "cm_leiden = pd.DataFrame(\n",
        "    confusion_matrix(true_labels, leiden_labels_comp),\n",
        "    index=[f\"True_{x}\" for x in sorted(set(true_labels))],\n",
        "    columns=[f\"Leiden_{x}\" for x in sorted(set(leiden_labels_comp))]\n",
        ")\n",
        "\n",
        "print(\"\\n=== Confusion Matrix: KMeans ===\")\n",
        "#print(cm_kmeans)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_kmeans, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix: True Labels vs K-Means Clusters\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.xlabel(\"Kmeans Clusters\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\n=== Confusion Matrix: Leiden ===\")\n",
        "#print(cm_leiden)\n",
        "plt.figure(figsize=(8,6))\n",
        "sns.heatmap(cm_leiden, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "plt.title(\"Confusion Matrix: True Labels vs K-Means Clusters\")\n",
        "plt.ylabel(\"True Labels\")\n",
        "plt.xlabel(\"Kmeans Clusters\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "506qcGDn-IvF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Align using Algorithm and compare True vs False\n",
        "\n",
        "\n",
        "#### Assign labels ####\n",
        "# True labels\n",
        "true_labels = np.array(labels_encode)\n",
        "print(f\"# of labels in true: {np.unique(true_labels)}\")\n",
        "\n",
        "# Predicted clusters\n",
        "kmeans_labels = kmeans.labels_\n",
        "print(f\"# of k-means labels: {np.unique(kmeans_labels)}\")\n",
        "leiden_labels = leiden_labels # from Leiden\n",
        "print(f\"# of leiden labels: {np.unique(leiden_labels)}\")\n",
        "\n",
        "#### Optimal label mapping using Hungarian algorithm ####\n",
        "def map_clusters_to_labels(true, pred):\n",
        "    cm = confusion_matrix(true, pred)\n",
        "    row_ind, col_ind = linear_sum_assignment(-cm)  # maximize matches\n",
        "    mapping = dict(zip(col_ind, row_ind))\n",
        "    mapped_pred = np.array([mapping[x] for x in pred])\n",
        "    return mapped_pred\n",
        "\n",
        "kmeans_labels_mapped = map_clusters_to_labels(true_labels, kmeans_labels)\n",
        "leiden_labels_mapped = map_clusters_to_labels(true_labels, leiden_labels)\n",
        "\n",
        "# ==============================\n",
        "# 3. Metrics\n",
        "# ==============================\n",
        "print(\"=== Adjusted Rand Index (ARI) ===\")\n",
        "print(\"KMeans vs Actual:\", adjusted_rand_score(true_labels, kmeans_labels))\n",
        "print(\"Leiden vs Actual:\", adjusted_rand_score(true_labels, leiden_labels))\n",
        "\n",
        "print(\"\\n=== Normalized Mutual Information (NMI) ===\")\n",
        "print(\"KMeans vs Actual:\", normalized_mutual_info_score(true_labels, kmeans_labels))\n",
        "print(\"Leiden vs Actual:\", normalized_mutual_info_score(true_labels, leiden_labels))\n",
        "\n",
        "# ==============================\n",
        "# 4. Confusion matrices\n",
        "# ==============================\n",
        "def plot_confusion(true, pred, title):\n",
        "    cm = pd.DataFrame(\n",
        "        confusion_matrix(true, pred),\n",
        "        index=[f\"True_{x}\" for x in sorted(set(true))],\n",
        "        columns=[f\"Pred_{x}\" for x in sorted(set(pred))]\n",
        "    )\n",
        "    plt.figure(figsize=(8,6))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
        "    plt.title(title)\n",
        "    plt.ylabel(\"True Labels\")\n",
        "    plt.xlabel(\"Predicted Clusters\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    return cm\n",
        "\n",
        "true_labels_plotting = le.inverse_transform(true_labels)\n",
        "kmeans_labels_mapped_plotting = le.inverse_transform(kmeans_labels_mapped)\n",
        "leiden_labels_mapped_plotting = le.inverse_transform(leiden_labels_mapped)\n",
        "\n",
        "cm_kmeans = plot_confusion(true_labels_plotting, kmeans_labels_mapped_plotting, \"Confusion Matrix: True vs KMeans\")\n",
        "cm_leiden = plot_confusion(true_labels_plotting, leiden_labels_mapped_plotting, \"Confusion Matrix: True vs Leiden\")\n",
        "\n",
        "#### Cluster purity (optional) ####\n",
        "def cluster_purity(true, pred):\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "    results = []\n",
        "    for c in np.unique(pred):\n",
        "        idx = np.where(pred == c)[0]\n",
        "        true_subset = true[idx]\n",
        "        majority_label = pd.Series(true_subset).mode()[0]\n",
        "        purity = (true_subset == majority_label).mean()\n",
        "        results.append([c, len(idx), majority_label, purity])\n",
        "    return pd.DataFrame(results, columns=[\"Cluster\", \"Size\", \"Majority Label\", \"Purity\"])\n",
        "\n",
        "print(\"\\n=== KMeans Cluster Purity ===\")\n",
        "print(cluster_purity(true_labels_plotting, kmeans_labels_mapped_plotting))\n",
        "\n",
        "print(\"\\n=== Leiden Cluster Purity ===\")\n",
        "print(cluster_purity(true_labels_plotting, leiden_labels_mapped_plotting))"
      ],
      "metadata": {
        "cellView": "form",
        "id": "KmN-CUw0IF0R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # UMAP plotting for gene and genotype together\n",
        "\n",
        "### Drop Keys and object columns ###\n",
        "umap_df = df_all.copy()\n",
        "\n",
        "geno_type = [\"WT\", \"HET\"]\n",
        "umap_df = umap_df[umap_df['genotype'].isin(geno_type)]\n",
        "\n",
        "# get gene labels to plot by gene\n",
        "umap_df[\"gene\"] = le.inverse_transform(umap_df[\"gene\"])\n",
        "\n",
        "# Create a column that mixes genotype and gene\n",
        "umap_df['gene_genotype'] = umap_df['gene'] + \"_\" + umap_df['genotype']\n",
        "\n",
        "\n",
        "# Find sex for labeling later\n",
        "gender_labels = umap_df['sex']\n",
        "\n",
        "# get gene labels to plot by gene\n",
        "#labels = le.inverse_transform(umap_df[\"gene\"])\n",
        "labels = umap_df[\"gene_genotype\"]\n",
        "umap_df.drop('gene_genotype', axis = 1, inplace=True)\n",
        "\n",
        "\n",
        "# Drop the columns that are needed\n",
        "umap_df = umap_df.drop(columns = merge_keys)\n",
        "print(merge_keys)\n",
        "\n",
        "# Ensure columns have been dropped\n",
        "drops = umap_df.select_dtypes(include=['object', 'bool']).columns\n",
        "umap_df.drop(columns = drops, inplace=True)\n",
        "\n",
        "\n",
        "# features to scale\n",
        "features = umap_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "### Standardize the scale ###\n",
        "x = umap_df.loc[:, features].values\n",
        "x = StandardScaler().fit_transform(x)\n",
        "\n",
        "feat_cols = ['feature'+str(i) for i in range(x.shape[1])]\n",
        "umap_df = pd.DataFrame(x, columns=feat_cols)\n",
        "\n",
        "\n",
        "umap_df.head()\n",
        "\n",
        "\n",
        "### Create the UMAP embeddings ###\n",
        "reducer = umap.UMAP(\n",
        "    n_neighbors=15,\n",
        "    min_dist=0.1,\n",
        "    metric=\"euclidean\",\n",
        "    random_state=42\n",
        ")\n",
        "embedding = reducer.fit_transform(umap_df)\n",
        "\n",
        "\n",
        "# Convert gene labels to colors\n",
        "unique_genes = np.unique(labels)\n",
        "palette = sns.color_palette(\"Paired\", len(unique_genes))\n",
        "gene_color_map = dict(zip(unique_genes, palette))\n",
        "\n",
        "# Male / Female masks\n",
        "male_mask = (gender_labels == 'M')\n",
        "female_mask = (gender_labels == 'F')\n",
        "\n",
        "plt.figure(figsize=(8,6))\n",
        "\n",
        "### Male UMAP = filled points ###\n",
        "plt.scatter(\n",
        "    embedding[male_mask, 0],\n",
        "    embedding[male_mask, 1],\n",
        "    s=60,\n",
        "    c=[gene_color_map[g] for g in labels[male_mask]],\n",
        "    #edgecolors='black',\n",
        "    #linewidths=0.5,\n",
        "    label=\"Male (filled)\"\n",
        ")\n",
        "\n",
        "### female UMAP = hollow points ###\n",
        "plt.scatter(\n",
        "    embedding[female_mask, 0],\n",
        "    embedding[female_mask, 1],\n",
        "    s=60,\n",
        "    facecolors='none',\n",
        "    edgecolors=[gene_color_map[g] for g in labels[female_mask]],\n",
        "    linewidths=3.0,\n",
        "    label=\"Female (hollow)\"\n",
        ")\n",
        "\n",
        "# Gene color legend\n",
        "for g in unique_genes:\n",
        "    plt.scatter([], [], color=gene_color_map[g], label=g)\n",
        "plt.legend(title=\"Gene\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "### Plot the UMAP combied ###\n",
        "plt.title(f\"UMAP: Colored by Gene and Genotype, Filled by Sex\")\n",
        "plt.xlabel(\"UMAP-1\")\n",
        "plt.ylabel(\"UMAP-2\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "dAUsLacIv5ls"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Conduct mutliple UMAPS based on different tests\n",
        "\n",
        "# create a copy of the meta table\n",
        "source_df = df_meta.copy()\n",
        "\n",
        "# define the columns we are zgoing to keep\n",
        "keep_cols = ['gene', 'sex', 'genotype']\n",
        "\n",
        "# Iterate through all the test type to generate umaps\n",
        "for source in source_df['session_type'].unique():\n",
        "    print(f\"Sourcing UMAP from: {source}\" )\n",
        "\n",
        "    # Make sure umap_df has only columns from the particular session type\n",
        "    cols_for_source = source_df.loc[\n",
        "        source_df['session_type'] == source,\n",
        "        'column_name'\n",
        "    ].tolist()\n",
        "\n",
        "   # Keep only matching columns (and optional metadata columns)\n",
        "    selected_cols = [c for c in cols_for_source if c in df_all.columns]\n",
        "    umap_df = df_all[keep_cols + selected_cols].copy()\n",
        "\n",
        "    print(\"Columns kept:\", keep_cols, selected_cols)\n",
        "\n",
        "    # keep only HET mice\n",
        "    geno_type = \"HET\"\n",
        "    umap_df = umap_df[umap_df['genotype'] == geno_type]\n",
        "\n",
        "    # Find sex for labeling later\n",
        "    gender_labels = umap_df['sex']\n",
        "\n",
        "    # get gene labels to plot by gene\n",
        "    labels = le.inverse_transform(umap_df[\"gene\"])\n",
        "\n",
        "    # Drop the columns that are labels\n",
        "    umap_df = umap_df.drop(columns = keep_cols)\n",
        "\n",
        "    # Ensure columns have been dropped\n",
        "    drops = umap_df.select_dtypes(include=['object', 'bool']).columns\n",
        "    umap_df.drop(columns = drops, inplace=True)\n",
        "\n",
        "    # features to scale\n",
        "    features = umap_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    ### Standardize the scale ###\n",
        "    x = umap_df.loc[:, features].values\n",
        "    x = StandardScaler().fit_transform(x)\n",
        "\n",
        "    feat_cols = ['feature'+str(i) for i in range(x.shape[1])]\n",
        "    umap_df = pd.DataFrame(x, columns=feat_cols)\n",
        "\n",
        "    ### Create the UMAP embeddings ###\n",
        "    reducer = umap.UMAP(\n",
        "        n_neighbors=15,\n",
        "        min_dist=0.1,\n",
        "        metric=\"euclidean\",\n",
        "        random_state=42\n",
        "    )\n",
        "    embedding = reducer.fit_transform(umap_df)\n",
        "\n",
        "\n",
        "    # Convert gene labels to colors\n",
        "    unique_genes = np.unique(labels)\n",
        "    palette = sns.color_palette(\"Paired\", len(unique_genes))\n",
        "    gene_color_map = dict(zip(unique_genes, palette))\n",
        "\n",
        "    # Male / Female masks\n",
        "    male_mask = (gender_labels == 'M')\n",
        "    female_mask = (gender_labels == 'F')\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    ### Male UMAP = filled points ###\n",
        "    plt.scatter(\n",
        "        embedding[male_mask, 0],\n",
        "        embedding[male_mask, 1],\n",
        "        s=60,\n",
        "        c=[gene_color_map[g] for g in labels[male_mask]],\n",
        "        #edgecolors='black',\n",
        "        #linewidths=0.5,\n",
        "        label=\"Male (filled)\"\n",
        "    )\n",
        "\n",
        "    ### female UMAP = hollow points ###\n",
        "    plt.scatter(\n",
        "        embedding[female_mask, 0],\n",
        "        embedding[female_mask, 1],\n",
        "        s=60,\n",
        "        facecolors='none',\n",
        "        edgecolors=[gene_color_map[g] for g in labels[female_mask]],\n",
        "        linewidths=3.0,\n",
        "        label=\"Female (hollow)\"\n",
        "    )\n",
        "\n",
        "    # Gene color legend\n",
        "    for g in unique_genes:\n",
        "        plt.scatter([], [], color=gene_color_map[g], label=g)\n",
        "    plt.legend(title=\"Gene\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    ### Plot the UMAP combied ###\n",
        "    plt.title(f\"UMAP: Colored by Gene, Filled by Sex, for {geno_type}, Session Type: {source}\")\n",
        "    plt.xlabel(\"UMAP-1\")\n",
        "    plt.ylabel(\"UMAP-2\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "5gBL5CSL1HP-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Conduct mutliple UMAPS based on different tests plotting both Gene and Genotype\n",
        "\n",
        "# create a copy of the meta table\n",
        "source_df = df_meta.copy()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# define the columns we are going to keep for now\n",
        "keep_cols = ['gene', 'sex', 'genotype']\n",
        "\n",
        "# Iterate through all the test type to generate umaps\n",
        "for source in source_df['session_type'].unique():\n",
        "    print(f\"Sourcing UMAP from: {source}\" )\n",
        "\n",
        "    # Make sure umap_df has only columns from the particular session type\n",
        "    cols_for_source = source_df.loc[\n",
        "        source_df['session_type'] == source,\n",
        "        'column_name'\n",
        "    ].tolist()\n",
        "\n",
        "\n",
        "\n",
        "   # Keep only matching columns (and optional metadata columns)\n",
        "    selected_cols = [c for c in cols_for_source if c in df_all.columns]\n",
        "    umap_df = df_all[keep_cols + selected_cols].copy()\n",
        "\n",
        "    print(\"Columns kept:\", keep_cols, selected_cols)\n",
        "\n",
        "    # get gene labels to plot by gene\n",
        "    umap_df[\"gene\"] = le.inverse_transform(umap_df[\"gene\"])\n",
        "\n",
        "    # Create a column that mixes genotype and gene\n",
        "    umap_df['gene_genotype'] = umap_df['gene'] + \"_\" + umap_df['genotype']\n",
        "\n",
        "\n",
        "    # Find sex for labeling later\n",
        "    gender_labels = umap_df['sex']\n",
        "\n",
        "    # get gene labels to plot by gene\n",
        "    #labels = le.inverse_transform(umap_df[\"gene\"])\n",
        "    labels = umap_df[\"gene_genotype\"]\n",
        "    umap_df.drop(columns = [\"gene_genotype\"], inplace=True)\n",
        "\n",
        "    # Drop the columns that are labels\n",
        "    umap_df = umap_df.drop(columns = keep_cols)\n",
        "\n",
        "    # Ensure columns have been dropped\n",
        "    drops = umap_df.select_dtypes(include=['object', 'bool']).columns\n",
        "    umap_df.drop(columns = drops, inplace=True)\n",
        "\n",
        "    # features to scale\n",
        "    features = umap_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "\n",
        "    ### Standardize the scale ###\n",
        "    x = umap_df.loc[:, features].values\n",
        "    x = StandardScaler().fit_transform(x)\n",
        "\n",
        "    feat_cols = ['feature'+str(i) for i in range(x.shape[1])]\n",
        "    umap_df = pd.DataFrame(x, columns=feat_cols)\n",
        "\n",
        "    ### Create the UMAP embeddings ###\n",
        "    reducer = umap.UMAP(\n",
        "        n_neighbors=15,\n",
        "        min_dist=0.1,\n",
        "        metric=\"euclidean\",\n",
        "        random_state=42\n",
        "    )\n",
        "    embedding = reducer.fit_transform(umap_df)\n",
        "\n",
        "\n",
        "    # Convert gene labels to colors\n",
        "    unique_genes = np.unique(labels)\n",
        "    palette = sns.color_palette(\"Paired\", len(unique_genes))\n",
        "    gene_color_map = dict(zip(unique_genes, palette))\n",
        "\n",
        "    # Male / Female masks\n",
        "    male_mask = (gender_labels == 'M')\n",
        "    female_mask = (gender_labels == 'F')\n",
        "\n",
        "    plt.figure(figsize=(8,6))\n",
        "\n",
        "    ### Male UMAP = filled points ###\n",
        "    plt.scatter(\n",
        "        embedding[male_mask, 0],\n",
        "        embedding[male_mask, 1],\n",
        "        s=60,\n",
        "        c=[gene_color_map[g] for g in labels[male_mask]],\n",
        "        #edgecolors='black',\n",
        "        #linewidths=0.5,\n",
        "        label=\"Male (filled)\"\n",
        "    )\n",
        "\n",
        "    ### female UMAP = hollow points ###\n",
        "    plt.scatter(\n",
        "        embedding[female_mask, 0],\n",
        "        embedding[female_mask, 1],\n",
        "        s=60,\n",
        "        facecolors='none',\n",
        "        edgecolors=[gene_color_map[g] for g in labels[female_mask]],\n",
        "        linewidths=3.0,\n",
        "        label=\"Female (hollow)\"\n",
        "    )\n",
        "\n",
        "    # Gene color legend\n",
        "    for g in unique_genes:\n",
        "        plt.scatter([], [], color=gene_color_map[g], label=g)\n",
        "    plt.legend(title=\"Gene\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "\n",
        "    ### Plot the UMAP combied ###\n",
        "    plt.title(f\"UMAP: Colored by Gene, Filled by Sex | Session Type: {source}\")\n",
        "    plt.xlabel(\"UMAP-1\")\n",
        "    plt.ylabel(\"UMAP-2\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "collapsed": true,
        "cellView": "form",
        "id": "SVuR77r2PQ9o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title # Conduct PCA Plotting Colored by Various Categoricals\n",
        "# make a copy from the df_all\n",
        "pca_df = df_all.copy()\n",
        "\n",
        "print(pca_df['genotype'].unique())\n",
        "\n",
        "# keep only HETs\n",
        "geno_type = \"HET\"\n",
        "pca_df = pca_df[pca_df['genotype'] == geno_type]\n",
        "\n",
        "# get gene labels to plot by gene\n",
        "pca_labels_encode = pca_df[\"gene\"]\n",
        "pca_labels = le.inverse_transform(pca_df[\"gene\"])\n",
        "\n",
        "\n",
        "# Drop categoricals\n",
        "drops = pca_df.select_dtypes(include=['object', 'bool']).columns\n",
        "pca_df.drop(columns = drops, inplace=True)\n",
        "\n",
        "# find most important features\n",
        "##non_feat = feat_imp[feat_imp['importance'] < 0.02]\n",
        "#non_feat_drops = list(non_feat['feature'])\n",
        "#print(non_feat_drops)\n",
        "#cols_to_drop = [c for c in non_feat_drops if c in pca_df.columns]\n",
        "#pca_df.drop(columns=cols_to_drop, inplace=True)\n",
        "\n",
        "features = pca_df.select_dtypes(include=['int64', 'float64']).columns\n",
        "print(len(features))\n",
        "\n",
        "### Standardize the scale ###\n",
        "x = pca_df.loc[:, features].values\n",
        "x = StandardScaler().fit_transform(x)\n",
        "feat_cols = ['feature'+str(i) for i in range(x.shape[1])]\n",
        "pca_df = pd.DataFrame(x, columns=feat_cols)\n",
        "\n",
        "### PCA Plotting ###\n",
        "pca = PCA(n_components=2)\n",
        "clustering_df_pca = pca.fit_transform(pca_df)\n",
        "clustering_df_pca = pd.DataFrame(data = clustering_df_pca,\n",
        "                                 columns = ['principal component 1', 'principal component 2'])\n",
        "print('Explained variability per principal component: {}'.format(pca.explained_variance_ratio_))\n",
        "\n",
        "unique_genes_pca = np.unique(pca_labels)\n",
        "palette = sns.color_palette(\"Paired\", len(unique_genes_pca))\n",
        "gene_color_map_pca = dict(zip(unique_genes_pca, palette))\n",
        "\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(clustering_df_pca[:, 0], clustering_df_pca[:, 1],\n",
        "            c=unique_genes_pca, cmap='viridis', s=30)\n",
        "plt.title('K-Means Clusters (PCA 2D Projection)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "# Colummns for coloring\n",
        "categorical_columns = ['gene']\n",
        "\n",
        "for cat_col in categorical_columns:\n",
        "    plt.figure(figsize=(6,6))\n",
        "    plt.title(f\"PCA colored by {cat_col}\", fontsize=16)\n",
        "    plt.xlabel(\"Principal Component 1\", fontsize=14)\n",
        "    plt.ylabel(\"Principal Component 2\", fontsize=14)\n",
        "\n",
        "    ### Get unique groups for column ###\n",
        "    targets = df_all[cat_col].dropna().unique()\n",
        "\n",
        "    # Generate colors\n",
        "    cmap = plt.cm.get_cmap('Paired', len(targets))\n",
        "    colors = [cmap(i) for i in range(len(targets))]\n",
        "\n",
        "    # Scatter for each group\n",
        "    for i, target in enumerate(targets):\n",
        "        indicesToKeep = df_all[cat_col] == target\n",
        "        plt.scatter(\n",
        "            clustering_df_pca.loc[indicesToKeep, 'principal component 1'],\n",
        "            clustering_df_pca.loc[indicesToKeep, 'principal component 2'],\n",
        "            color=colors[i],\n",
        "            s=50,\n",
        "            alpha=0.7,\n",
        "            edgecolors='k',\n",
        "            label=str(target)\n",
        "        )\n",
        "    plt.legend(title=cat_col, fontsize=9, title_fontsize=11)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\"\"\""
      ],
      "metadata": {
        "collapsed": true,
        "id": "hSKHHmV1Mtll",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Perform unsupervised PCA  k-means & KNN clustering\n",
        "kmeans = KMeans(n_clusters=5, random_state=42)\n",
        "labels = kmeans.fit_predict(pca_df)\n",
        "\n",
        "# PCA for visualization\n",
        "pca = PCA(n_components=2)\n",
        "clustering_df_pca = pca.fit_transform(pca_df)\n",
        "\n",
        "# Plot our pca data colored by clusters\n",
        "plt.figure(figsize=(4,4))\n",
        "plt.scatter(clustering_df_pca[:, 0], clustering_df_pca[:, 1], c=labels,\n",
        "            cmap='viridis', s=30)\n",
        "plt.title('K-Means Clusters (PCA 2D Projection)')\n",
        "plt.xlabel('Principal Component 1')\n",
        "plt.ylabel('Principal Component 2')\n",
        "\n",
        "# leiden - Build kNN graph\n",
        "k = 5\n",
        "knn_graph = kneighbors_graph(pca_df, k, mode='connectivity', include_self=False)\n",
        "sources, targets = knn_graph.nonzero()\n",
        "\n",
        "# Create igraph object\n",
        "g = ig.Graph(edges=list(zip(sources, targets)), directed=False)\n",
        "g.simplify()\n",
        "\n",
        "# Iterate thteough resplutions to find clustering for 5 groups\n",
        "for res in [0.2, 0.4, 0.6, 0.8, 1.0, 1.2]:\n",
        "    part = leidenalg.find_partition(\n",
        "        g,\n",
        "        leidenalg.RBConfigurationVertexPartition,\n",
        "        resolution_parameter=res\n",
        "    )\n",
        "    print(res, len(set(part.membership)))\n",
        "\n",
        "res = 1.2\n",
        "# run Leiden partitioning\n",
        "partition = leidenalg.find_partition(\n",
        "    g,\n",
        "    leidenalg.RBConfigurationVertexPartition,\n",
        "    resolution_parameter = res\n",
        ")\n",
        "leiden_labels = np.array(partition.membership)\n",
        "\n",
        "# Partition\n",
        "partition = leidenalg.find_partition(\n",
        "    g,\n",
        "    leidenalg.RBConfigurationVertexPartition,\n",
        "    resolution_parameter=1  # fewer clusters\n",
        ")\n"
      ],
      "metadata": {
        "id": "yrqfTcVvsiYu",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}